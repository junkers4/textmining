{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de9fb18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.9.1)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: spacy in /usr/local/python/3.12.1/lib/python3.12/site-packages (3.8.5)\n",
      "Requirement already satisfied: stanza in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.10.1)\n",
      "Requirement already satisfied: click in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (0.15.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (2.11.4)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (76.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: emoji in /usr/local/python/3.12.1/lib/python3.12/site-packages (from stanza) (2.14.1)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from stanza) (6.31.0)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from stanza) (3.3)\n",
      "Requirement already satisfied: torch>=1.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from stanza) (2.6.0+cpu)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.3.0->stanza) (3.13.1)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.3.0->stanza) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.local/lib/python3.12/site-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/python/3.12.1/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk pandas spacy stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a41dc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "# Aplikácia lemmatizácie\n",
    "df['lemmatized_tokens'] = df['tokens_no_stopwords'].apply(lambda tokens: lemmatize_text(' '.join(tokens)))\n",
    "print(\"\\nUkážka lemmatizovaných tokenov:\")\n",
    "print(df[['tokens_no_stopwords', 'lemmatized_tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4458185",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 165MB/s]                     \n",
      "2025-05-15 10:45:28 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-05-15 10:45:28 INFO: Downloading default packages for language: cs (Czech) ...\n",
      "2025-05-15 10:45:28 INFO: File exists: /home/codespace/stanza_resources/cs/default.zip\n",
      "2025-05-15 10:45:30 INFO: Finished downloading models and saved to /home/codespace/stanza_resources\n",
      "2025-05-15 10:45:30 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 182MB/s]                     \n",
      "2025-05-15 10:45:30 INFO: Downloaded file to /home/codespace/stanza_resources/resources.json\n",
      "2025-05-15 10:45:30 WARNING: Language cs package default expects mwt, which has been added\n",
      "2025-05-15 10:45:32 INFO: Loading these models for language: cs (Czech):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | pdt          |\n",
      "| mwt       | pdt          |\n",
      "| pos       | pdt_nocharlm |\n",
      "| lemma     | pdt_nocharlm |\n",
      "============================\n",
      "\n",
      "2025-05-15 10:45:32 INFO: Using device: cpu\n",
      "2025-05-15 10:45:32 INFO: Loading: tokenize\n",
      "2025-05-15 10:45:32 INFO: Loading: mwt\n",
      "2025-05-15 10:45:32 INFO: Loading: pos\n",
      "2025-05-15 10:45:34 INFO: Loading: lemma\n",
      "2025-05-15 10:45:36 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prvé riadky dát:\n",
      "   Hviezdičky             Meno                             Pocet_recenzii  \\\n",
      "0           1   Valerie Šálová                                  2 recenze   \n",
      "1           4   Václav Namyslo   Místní průvodce · 99 recenzí · 150 fotek   \n",
      "2           1              J P                                  1 recenze   \n",
      "3           4       Jan Franek   Místní průvodce · 106 recenzí · 33 fotek   \n",
      "4           5  Miroslav Vlasak  Místní průvodce · 241 recenzí · 135 fotek   \n",
      "\n",
      "                                                Text       Datum  \n",
      "0  Obsluha od pohledu velice nepříjemná, na výběr...  2025-05-12  \n",
      "1  Ale jo. Jídlo z menu bylo dobré. Akorát špenát...  2025-05-15  \n",
      "2  Čekali jsme velmi dlouho, obsluha nepříjemná. ...  2025-05-15  \n",
      "3  Příjemná restaurace!\\r\\n\\r\\nDo restaurace jsem...  2024-09-17  \n",
      "4  Měli jsme syrečkový tatarák a s mažené gorbáči...  2025-05-15  \n",
      "\n",
      "Základné informácie o dátach:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 258 entries, 0 to 257\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Hviezdičky      258 non-null    int64 \n",
      " 1   Meno            258 non-null    object\n",
      " 2   Pocet_recenzii  258 non-null    object\n",
      " 3   Text            258 non-null    object\n",
      " 4   Datum           258 non-null    object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 10.2+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import stanza\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Inicializácia Stanza pre češtinu (na lemmatizáciu a POS tagging)\n",
    "stanza.download('cs')  # Stiahnutie českého modelu\n",
    "nlp = stanza.Pipeline(lang='cs', processors='tokenize,lemma,pos')\n",
    "\n",
    "# Stiahnutie NLTK dát\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Načítanie CSV súboru\n",
    "# Predpokladáme, že CSV má stĺpec 'text' s českými textovými dátami\n",
    "df = pd.read_csv('recenzie.csv')  # Nahraď 'data.csv' správnym názvom súboru\n",
    "print(\"Prvé riadky dát:\")\n",
    "print(df.head())\n",
    "\n",
    "# Základné štatistiky\n",
    "print(\"\\nZákladné informácie o dátach:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9880d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ukážka vyčisteného textu:\n",
      "                                                Text  \\\n",
      "0  Obsluha od pohledu velice nepříjemná, na výběr...   \n",
      "1  Ale jo. Jídlo z menu bylo dobré. Akorát špenát...   \n",
      "2  Čekali jsme velmi dlouho, obsluha nepříjemná. ...   \n",
      "3  Příjemná restaurace!\\r\\n\\r\\nDo restaurace jsem...   \n",
      "4  Měli jsme syrečkový tatarák a s mažené gorbáči...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  obsluha od pohledu velice nepříjemná, na výběr...  \n",
      "1  ale jo. jídlo z menu bylo dobré. akorát špenát...  \n",
      "2  čekali jsme velmi dlouho, obsluha nepříjemná. ...  \n",
      "3  příjemná restaurace! do restaurace jsem přišel...  \n",
      "4  měli jsme syrečkový tatarák a s mažené gorbáči...  \n"
     ]
    }
   ],
   "source": [
    "#Lowercase a čistenie textu\n",
    "\n",
    "def clean_text(text):\n",
    "    # Prevod na malé písmená\n",
    "    text = text.lower()\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Aplikácia čistenia na stĺpec 'text'\n",
    "df['cleaned_text'] = df['Text'].apply(clean_text)\n",
    "print(\"\\nUkážka vyčisteného textu:\")\n",
    "print(df[['Text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c41514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ukážka tokenizovaného textu:\n",
      "                                        cleaned_text  \\\n",
      "0  obsluha od pohledu velice nepříjemná, na výběr...   \n",
      "1  ale jo. jídlo z menu bylo dobré. akorát špenát...   \n",
      "2  čekali jsme velmi dlouho, obsluha nepříjemná. ...   \n",
      "3  příjemná restaurace! do restaurace jsem přišel...   \n",
      "4  měli jsme syrečkový tatarák a s mažené gorbáči...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [obsluha, od, pohledu, velice, nepříjemná, ,, ...  \n",
      "1  [ale, jo, ., jídlo, z, menu, bylo, dobré, ., a...  \n",
      "2  [čekali, jsme, velmi, dlouho, ,, obsluha, nepř...  \n",
      "3  [příjemná, restaurace, !, do, restaurace, jsem...  \n",
      "4  [měli, jsme, syrečkový, tatarák, a, s, mažené,...  \n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return [word.text for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "# Aplikácia tokenizácie\n",
    "df['tokens'] = df['cleaned_text'].apply(tokenize_text)\n",
    "print(\"\\nUkážka tokenizovaného textu:\")\n",
    "print(df[['cleaned_text', 'tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ae67004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ukážka tokenov po odstránení stop slov:\n",
      "                                              tokens  \\\n",
      "0  [obsluha, od, pohledu, velice, nepříjemná, ,, ...   \n",
      "1  [ale, jo, ., jídlo, z, menu, bylo, dobré, ., a...   \n",
      "2  [čekali, jsme, velmi, dlouho, ,, obsluha, nepř...   \n",
      "3  [příjemná, restaurace, !, do, restaurace, jsem...   \n",
      "4  [měli, jsme, syrečkový, tatarák, a, s, mažené,...   \n",
      "\n",
      "                                 tokens_no_stopwords  \n",
      "0  [obsluha, pohledu, velice, nepříjemná, ,, výbě...  \n",
      "1  [jo, ., jídlo, menu, dobré, ., akorát, špenát,...  \n",
      "2  [čekali, velmi, dlouho, ,, obsluha, nepříjemná...  \n",
      "3  [příjemná, restaurace, !, restaurace, přišel, ...  \n",
      "4  [měli, syrečkový, tatarák, mažené, gorbáčiky, ...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Načítanie stop slov z GitHubu\n",
    "# používame české stop slová z repozitára stopwords-iso link\n",
    "stopwords_url = 'https://raw.githubusercontent.com/stopwords-iso/stopwords-cs/master/stopwords-cs.json'\n",
    "response = requests.get(stopwords_url)\n",
    "czech_stop_words = set(json.loads(response.text))\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in czech_stop_words]\n",
    "\n",
    "# Odstránenie stop slov\n",
    "df['tokens_no_stopwords'] = df['tokens'].apply(remove_stopwords)\n",
    "print(\"\\nUkážka tokenov po odstránení stop slov:\")\n",
    "print(df[['tokens', 'tokens_no_stopwords']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1546ccfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ukážka lemmatizovaných tokenov:\n",
      "                                 tokens_no_stopwords  \\\n",
      "0  [obsluha, pohledu, velice, nepříjemná, ,, výbě...   \n",
      "1  [jo, ., jídlo, menu, dobré, ., akorát, špenát,...   \n",
      "2  [čekali, velmi, dlouho, ,, obsluha, nepříjemná...   \n",
      "3  [příjemná, restaurace, !, restaurace, přišel, ...   \n",
      "4  [měli, syrečkový, tatarák, mažené, gorbáčiky, ...   \n",
      "\n",
      "                                   lemmatized_tokens  \n",
      "0  [obsluha, pohled, velice, příjemný, ,, výběr, ...  \n",
      "1  [jo, ., jídlo, menu, dobrý, ., akorát, špenát,...  \n",
      "2  [čekat, velmi, dlouho, ,, obsluha, příjemný, ....  \n",
      "3  [příjemný, restaurace, !, restaurace, přijít, ...  \n",
      "4  [mít, syrečkový, tatarák, mažený, gorbáčika, ,...  \n"
     ]
    }
   ],
   "source": [
    "#Ceska lematizácia\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "\n",
    "# Aplikácia lemmatizácie\n",
    "df['lemmatized_tokens'] = df['tokens_no_stopwords'].apply(lambda tokens: lemmatize_text(' '.join(tokens)))\n",
    "print(\"\\nUkážka lemmatizovaných tokenov:\")\n",
    "print(df[['tokens_no_stopwords', 'lemmatized_tokens']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6782aee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ukážka TF-IDF matice:\n",
      "\n",
      "Ukážka TF-IDF matice:\n",
      "    00   10   11   12   13   14   15  170   18   20  ...  špinava  špinavej  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0       0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0       0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0       0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0       0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0       0.0   \n",
      "\n",
      "   špinavý  štěkat  štěstí     šunka  žampion  žebro  židlička  žádný  \n",
      "0      0.0     0.0     0.0  0.296158      0.0    0.0       0.0    0.0  \n",
      "1      0.0     0.0     0.0  0.000000      0.0    0.0       0.0    0.0  \n",
      "2      0.0     0.0     0.0  0.000000      0.0    0.0       0.0    0.0  \n",
      "3      0.0     0.0     0.0  0.000000      0.0    0.0       0.0    0.0  \n",
      "4      0.0     0.0     0.0  0.000000      0.0    0.0       0.0    0.0  \n",
      "\n",
      "[5 rows x 1000 columns]\n"
     ]
    }
   ],
   "source": [
    "# Inicializácia TF-IDF vektorizéra\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Obmedzíme na 1000 najčastejších slov\n",
    "tfidf_matrix = vectorizer.fit_transform(df['lemmatized_tokens'].apply(lambda x: ' '.join(x)))\n",
    "print(\"\\nUkážka TF-IDF matice:\")\n",
    "\n",
    "# Prevedenie na DataFrame pre lepšiu prehľadnosť\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"\\nUkážka TF-IDF matice:\")\n",
    "print(tfidf_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
